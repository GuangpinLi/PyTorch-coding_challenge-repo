{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Required Libraries**\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-14T20:12:39.529269Z","iopub.execute_input":"2023-10-14T20:12:39.529494Z","iopub.status.idle":"2023-10-14T20:12:44.848503Z","shell.execute_reply.started":"2023-10-14T20:12:39.529473Z","shell.execute_reply":"2023-10-14T20:12:44.847597Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import models, transforms, datasets\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport glob\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.790689Z","iopub.execute_input":"2023-10-16T21:26:03.791008Z","iopub.status.idle":"2023-10-16T21:26:03.797343Z","shell.execute_reply.started":"2023-10-16T21:26:03.790981Z","shell.execute_reply":"2023-10-16T21:26:03.796249Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"# Check if a GPU is available and set the device accordingly to ensures that the model and data are both on the GPU when available, \n# which can significantly accelerate training and inference for deep learning tasks.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.798951Z","iopub.execute_input":"2023-10-16T21:26:03.799881Z","iopub.status.idle":"2023-10-16T21:26:03.814440Z","shell.execute_reply.started":"2023-10-16T21:26:03.799852Z","shell.execute_reply":"2023-10-16T21:26:03.813482Z"},"trusted":true},"execution_count":172,"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Reading**\nRead the original datasets and create a dataframe containing the image paths and their corresponding labels","metadata":{}},{"cell_type":"code","source":"\nclass TumorDataset:\n    \"\"\"\n    This class is designed to read two types of original datasets and create a dataframe containing the image paths and their corresponding labels \n    \n    Attributes:\n        root_dir (str): The root directory where the two types of original datasets are located.\n        df_UID_label (pd.DataFrame): A dataframe with two columns, ['UID', 'label'].\n        df_image_str_label_path_file (pd.DataFrame): A dataframe with two columns, ['UID', 'Img_path_file'].\n        df_image_path_file_label (pd.DataFrame): A dataframe with two columns, ['Img_label', 'Img_path_file'].\n\n    Methods:\n        read_uid_label_file(): Reads the CSV file STAN_labels.csv and populates df_UID_label.\n        read_image_str_label_path_file(): Reads all image files and populates df_image_str_label_path_file.\n        derive_image_path_file_label(): Derives a dataframe with two columns, ['Img_label', 'Img_path_file'].\n    \"\"\"\n\n    def __init__(self, root_dir):\n        \"\"\"\n        Initialize a new object of the class.\n\n        Args:\n            root_dir (str): The root directory where two types of original datasets are located.\n\n        Attributes:\n            root_dir (str): The root directory where the datasets are located.\n            df_UID_label (DataFrame): A DataFrame for storing UID-label associations (initially None).\n            df_image_str_label_path_file (DataFrame): A DataFrame for storing image string label and path associations (initially None).\n        \"\"\"\n\n        self.root_dir = root_dir\n        self.df_UID_label = None  \n        self.df_image_str_label_path_file = None\n\n\n    def read_uid_label_file(self):\n        \"\"\"\n        Read the 'STAN_labels.csv' file and populate the 'df_UID_label' DataFrame.\n\n        Returns:\n            None\n\n        Note:\n            Ensure that the 'root_dir' attribute points to the directory containing the 'STAN_labels.csv' file.\n        \"\"\"\n\n        path_csv_file = self.root_dir+'/labels/STAN_labels.csv'\n        df_UID_label = pd.read_csv(path_csv_file)\n        self.df_UID_label = df_UID_label\n        \n\n    def read_image_str_label_path_file(self):\n        \"\"\"\n        Read and collect information from image files to populate the 'df_image_str_label_path_file' DataFrame.\n\n        This method scans the 'patches' directory within the specified 'root_dir' and collects information about image files.\n        It then populates the 'df_image_str_label_path_file' DataFrame with the UID (image identifier) and file paths.\n\n        Returns:\n            None\n\n        Note:\n            Ensure that the 'root_dir' attribute points to the directory containing the 'patches' subdirectory with image files.\n    \"\"\"\n\n        path = self.root_dir +'/patches'\n        img_str_label_list = []\n        img_path_file_list = []\n\n        for img_UID in os.listdir(path):\n            for img_path_file in glob.glob(path+'/'+img_UID+'/'+'*.png'):\n                img_str_label_list.append(img_UID)\n                img_path_file_list.append(img_path_file)\n\n        #form a dataframe with two columns, ['UID','Img_path_file']\n        df_image_str_label_path_file = pd.DataFrame(data={'UID':img_str_label_list, 'Img_path_file':img_path_file_list})\n        self.df_image_str_label_path_file = df_image_str_label_path_file\n\n    # Create a dataframe with two columns, ['Img_label','Img_path_file']\n    def derive_image_path_file_label(self)->pd.DataFrame:\n        \"\"\"\n        Derive a DataFrame with two columns, ['Img_label', 'Img_path_file'] based on existing data.\n\n        This method generates a new DataFrame with two columns: 'Img_label' and 'Img_path_file', by mapping image labels\n        from the 'df_UID_label' DataFrame to image paths and filenames in 'df_image_str_label_path_file'.\n\n        Returns:\n            pd.DataFrame: The new DataFrame with columns ['Img_label', 'Img_path_file'].\n\n        Note:\n            - Ensure that 'df_UID_label' and 'df_image_str_label_path_file' attributes are properly populated.\n            - The resulting DataFrame represents image labels and corresponding file paths.\n        \"\"\"\n\n        self.df_UID_label.rename(columns={'MUT_STATUS':'Img_label'}, inplace=True)\n        UID_label_dict = dict(zip(self.df_UID_label.UID,self.df_UID_label.Img_label))\n        df_image_path_file_label = self.df_image_str_label_path_file.copy()\n        df_image_path_file_label['UID']= self.df_image_str_label_path_file['UID'].map(UID_label_dict)\n        df_image_path_file_label.rename(columns={'UID':'Img_label'}, inplace=True)\n\n        return df_image_path_file_label\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.816491Z","iopub.execute_input":"2023-10-16T21:26:03.816834Z","iopub.status.idle":"2023-10-16T21:26:03.827691Z","shell.execute_reply.started":"2023-10-16T21:26:03.816799Z","shell.execute_reply":"2023-10-16T21:26:03.826825Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"markdown","source":"# **Perform Data Reading**","metadata":{}},{"cell_type":"code","source":"root_dir = '/kaggle/input/stan-datasets/STAN_patches_lbls/STAN_patches_lbls'\ntumor_dataset = TumorDataset(root_dir)\ntumor_dataset.read_uid_label_file()\ntumor_dataset.read_image_str_label_path_file()\ndf_image_path_file_label = tumor_dataset.derive_image_path_file_label()\ndf_image_path_file_label.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.828933Z","iopub.execute_input":"2023-10-16T21:26:03.829249Z","iopub.status.idle":"2023-10-16T21:26:03.928856Z","shell.execute_reply.started":"2023-10-16T21:26:03.829217Z","shell.execute_reply":"2023-10-16T21:26:03.927805Z"},"trusted":true},"execution_count":174,"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"   Img_label                                      Img_path_file\n0          1  /kaggle/input/stan-datasets/STAN_patches_lbls/...\n1          1  /kaggle/input/stan-datasets/STAN_patches_lbls/...\n2          1  /kaggle/input/stan-datasets/STAN_patches_lbls/...\n3          1  /kaggle/input/stan-datasets/STAN_patches_lbls/...\n4          1  /kaggle/input/stan-datasets/STAN_patches_lbls/...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Img_label</th>\n      <th>Img_path_file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>/kaggle/input/stan-datasets/STAN_patches_lbls/...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>/kaggle/input/stan-datasets/STAN_patches_lbls/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>/kaggle/input/stan-datasets/STAN_patches_lbls/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>/kaggle/input/stan-datasets/STAN_patches_lbls/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>/kaggle/input/stan-datasets/STAN_patches_lbls/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Preprocessing and Transformation**","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    A custom dataset class designed for data preprocessing and transformation for model training.\n\n    Attributes:\n        dataframe (pd.DataFrame): The DataFrame containing image path and label data.\n        transform (torchvision.transforms.Compose, optional): A PyTorch transformation pipeline for image processing.\n\n    Methods:\n        __len__(): Get the size of the dataset.\n        __getitem__(idx): Retrieve the specified image and apply the specified transformations.\n\n    \"\"\"\n\n    def __init__(self, dataframe, transform=None):\n        \"\"\"\n        Initialize a CustomDataset object.\n\n        Args:\n            dataframe (pd.DataFrame): The DataFrame containing image path and label data.\n            transform (torchvision.transforms.Compose, optional): A PyTorch transformation pipeline for image processing.\n        \"\"\"\n        \n        self.data = dataframe\n        self.transform = transform\n        \n\n    def __len__(self):\n        \"\"\"\n        Get the size of the dataset.\n\n        Returns:\n            int: The number of samples in the dataset.\n        \"\"\"\n\n        return len(self.data)\n    \n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get the specified image by an index and apply the transformation to it.\n\n        Args:\n            idx (int): The index of the image path.\n\n        Returns:\n            image (PIL.Image.Image): The image data.\n            label (int): The image label.\n        \"\"\"\n        \n        img_path = self.data.iloc[idx, 1]  # image_path column\n        image = Image.open(img_path).convert('RGB')\n        label = self.data.iloc[idx, 0]  # image_label column\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.931439Z","iopub.execute_input":"2023-10-16T21:26:03.932031Z","iopub.status.idle":"2023-10-16T21:26:03.939348Z","shell.execute_reply.started":"2023-10-16T21:26:03.931997Z","shell.execute_reply":"2023-10-16T21:26:03.938440Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"markdown","source":"# **Perform Data Preprocessing and Transformation**","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Resize the image (224, 224)\n    transforms.ToTensor(),          # Convert to tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image\n])\ncustom_dataset = CustomDataset(dataframe=df_image_path_file_label, transform=transform)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.940853Z","iopub.execute_input":"2023-10-16T21:26:03.941482Z","iopub.status.idle":"2023-10-16T21:26:03.954644Z","shell.execute_reply.started":"2023-10-16T21:26:03.941450Z","shell.execute_reply":"2023-10-16T21:26:03.953668Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":"# **Data Splitting**\nPartition the dataset into training, validation, and testing subsets.","metadata":{}},{"cell_type":"code","source":"# Split the dataset to get train_dataset, val_dataset and test_dataset\ntrain_size = int(0.7 * len(custom_dataset))\nval_size = int(0.15 * len(custom_dataset))\ntest_size = len(custom_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, val_size, test_size])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.956224Z","iopub.execute_input":"2023-10-16T21:26:03.956602Z","iopub.status.idle":"2023-10-16T21:26:03.971617Z","shell.execute_reply.started":"2023-10-16T21:26:03.956572Z","shell.execute_reply":"2023-10-16T21:26:03.970762Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"# Define the custom CNN model\n\nclass TumorClassifier(nn.Module):\n    \"\"\"\n    TumorClassifier is a convolutional neural network (CNN) model designed for tumor classification tasks.\n\n    Attributes:\n        - conv1 (nn.Conv2d): The first convolutional layer with 3 input channels and 32 output channels.\n        - pool (nn.MaxPool2d): A max-pooling layer with a kernel size of 2 and a stride of 2.\n        - conv2 (nn.Conv2d): The second convolutional layer with 32 input channels and 64 output channels.\n        - fc1 (nn.Linear): The first fully connected layer with 64 * 32 * 32 input features and 128 output features.\n        - fc2 (nn.Linear): The second fully connected layer with 128 input features and 2 output features.\n\n    Methods:\n        - forward(x): Performs a forward pass through the network.\n\n            Args:\n                - x (torch.Tensor): The input data tensor of shape (batch_size, 3, height, width).\n\n            Returns:\n                - torch.Tensor: The output tensor representing class scores. Shape depends on the number of classes.\n\n    Example:\n        # Create an instance of the model\n        model = TumorClassifier().to(device) \n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the TumorClassifier neural network.\n\n        This constructor sets up the TumorClassifier network architecture by defining its layers and parameters.\n\n        Architecture:\n            - Convolutional Layer 1 (conv1): 3 input channels, 32 output channels, kernel size 3x3, padding 1.\n            - Max-Pooling Layer (pool): Kernel size 2x2, stride 2.\n            - Convolutional Layer 2 (conv2): 32 input channels, 64 output channels, kernel size 3x3, padding 1.\n            - Fully Connected Layer 1 (fc1): Input size 64 x 32 x 32, output size 128.\n            - Fully Connected Layer 2 (fc2): Input size 128, output size 2 (corresponding to the number of classes).\n\n        Usage:\n            Instantiate this class to create a TumorClassifier neural network for classification tasks.\n        \"\"\"\n        \n        super(TumorClassifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n        self.fc2 = nn.Linear(128, 2)\n\n    \n    def forward(self, x):\n        \"\"\"\n        Perform a forward pass through the TumorClassifier model.\n\n        This method takes the input data tensor 'x' and passes it through the network to compute class scores.\n    \n        Args:\n            x (torch.Tensor): The input data tensor of shape (batch_size, 3, height, width).\n\n        Returns:\n            torch.Tensor: The output tensor representing class scores. The shape depends on the number of classes.\n        \"\"\"\n        \n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 32 * 32)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.975234Z","iopub.execute_input":"2023-10-16T21:26:03.975783Z","iopub.status.idle":"2023-10-16T21:26:03.990852Z","shell.execute_reply.started":"2023-10-16T21:26:03.975753Z","shell.execute_reply":"2023-10-16T21:26:03.989888Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"markdown","source":"# **Create a class of model training and evaluation**","metadata":{}},{"cell_type":"code","source":"class ModelTrainingEvaluation:\n    \"\"\"\n    A class for designing model architecture, training the model, and evaluating it on the test set.\n\n    Attributes:\n        learning_rate (float): The learning rate hyperparameter for model training.\n        batch_size (int): The size of each batch during training.\n        num_epochs (int): The total number of training epochs.\n        pretrained (bool): Flag to indicate whether to use a pre-trained model.\n\n    Methods:\n        create_initial_model():\n            Define the model architecture and essential components.\n\n        create_3_loaders():\n            Create data loaders for training, validation, and testing sets.\n\n        evaluate_model(model, dataloader, if_plot_conf_mat=False):\n            Evaluate the model's performance on a given dataset.\n\n        train_model(model, optimizer, criterion, scheduler, num_epochs, train_loader, validation_loader):\n            Train the model using the specified settings.\n\n        save_final_model(model):\n            Save the final trained model to a file.\n    \"\"\"\n\n    def __init__(self, learning_rate, batch_size, num_epochs, pretrained=False):\n        \"\"\"\n        Initialize a ModelTrainingEvaluation object.\n\n        Args:\n            learning_rate (float): The learning rate hyperparameter for model training.\n            batch_size (int): The size of each batch during training.\n            num_epochs (int): The total number of training epochs.\n            pretrained (bool): Flag to indicate whether to use a pre-trained model (default is False).\n        \"\"\"\n        \n        self.pretrained = pretrained \n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n\n\n    # Define the model architecture\n    def create_initial_model(self):\n        \"\"\"\n        Define the model architecture and essential components.\n\n        Returns:\n            model (torch.nn.Module): The initial model.\n            optimizer (torch.optim.Optimizer): Adam optimizer.\n            criterion (torch.nn.Module): Loss function for training.\n            scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler for training.\n        \"\"\"\n      \n        if self.pretrained:    \n            model = models.resnet18(pretrained=True)\n            num_ftrs = model.fc.in_features\n            model.fc = nn.Linear(num_ftrs, 2)  # Modify the final classification layer for binary classification\n            model = model.to(device) # Move the model to the GPU if available\n        else:\n            model = TumorClassifier().to(device)\n            \n        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n        criterion = nn.CrossEntropyLoss()\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduling\n        return model, optimizer, criterion, scheduler\n\n    # Create 3 data loaders\n    def create_3_loaders(self):\n        \"\"\"\n        Create data loaders for training, validation, and testing sets.\n\n        Returns:\n            train_loader: Iterator for training set batches.\n            validation_loader: Iterator for validation set batches.\n            test_loader: Iterator for testing set batches.\n        \"\"\"\n\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n        validation_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True)\n        return train_loader, validation_loader, test_loader\n\n    # Evaluate model\n    def evaluate_model(self, model, dataloader, if_plot_conf_mat=False):\n        \"\"\"\n        Evaluate the model's performance on a specified dataset.\n\n        Args:\n            model (torch.nn.Module): The model to evaluate.\n            dataloader (torch.utils.data.DataLoader): DataLoader for the evaluation dataset.\n            if_plot_conf_mat (bool, optional): If True, plot the confusion matrix; otherwise, do not plot.\n\n        Returns:\n            accuracy (float): The accuracy of the model on the given dataset.\n        \"\"\"\n\n        model.eval()\n        correct = 0\n        total = 0\n        if if_plot_conf_mat:\n            predictions = torch.tensor([]).to(device)\n            true_labels = torch.tensor([]).to(device)\n        with torch.no_grad():\n            for inputs, labels in dataloader:\n                inputs, labels = inputs.to(device), labels.to(device)  # Move data to the GPU\n                outputs = model(inputs)\n                _, predicted = torch.max(outputs, 1)\n                if if_plot_conf_mat:\n                    predictions = torch.cat((predictions, predicted), dim=0)\n                    true_labels = torch.cat((true_labels, labels), dim=0)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        accuracy = correct / total\n\n        if if_plot_conf_mat:\n          conf_matrix = confusion_matrix(true_labels.cpu(), predictions.cpu())\n          # Plot the confusion matrix\n          sns.heatmap(conf_matrix, annot=True, fmt=\"d\")\n          plt.xlabel('Predicted')\n          plt.ylabel('True')\n          plt.title(\"Confusion Matrix for Test Set\")\n          plt.show()\n        return accuracy\n\n    def train_model(self, model, optimizer, criterion, scheduler, num_epochs, train_loader, validation_loader):\n        \"\"\"\n        Train the model using the specified settings.\n\n        Args:\n            model (pre-trained or torch.nn.Module): The model to tune or train.\n            optimizer (torch.optim.Optimizer): The optimizer for training.\n            criterion (torch.nn.Module): Loss function for training.\n            scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler for training.\n            num_epochs (int): The total number of training epochs.\n            train_loader (torch.utils.data.DataLoader): Iterator for training set batches.\n            validation_loader (torch.utils.data.DataLoader): Iterator for validation set batches.\n\n        Returns:\n            model (torch.nn.Module): The tuned or trained model.\n        \"\"\"\n        for epoch in range(self.num_epochs):\n            model.train()\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)  # Move data to the GPU\n                optimizer.zero_grad()\n                outputs = model(inputs).to(device)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n            scheduler.step()  # Adjust learning rate\n            # Evaluate the model on the validation set\n            validation_accuracy = self.evaluate_model(model, validation_loader, False)\n            print(f'Epoch [{epoch+1}/{num_epochs}] - Validation Accuracy: {100 * validation_accuracy:.2f}%')\n            print(f'Epoch, Loss: {epoch+1, loss.item()}')\n        return model\n\n    def save_final_model(self, model):\n        \"\"\"\n        Save the final trained model to a file.\n\n        Args:\n            model: The final trained model.\n        \"\"\"    \n        \n        torch.save(model.state_dict(), 'final_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:03.995222Z","iopub.execute_input":"2023-10-16T21:26:03.996806Z","iopub.status.idle":"2023-10-16T21:26:04.027809Z","shell.execute_reply.started":"2023-10-16T21:26:03.996772Z","shell.execute_reply":"2023-10-16T21:26:04.026623Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"markdown","source":"# **Perform model training and evaluation**","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\nlearning_rate, batch_size, num_epochs = 0.001, 32, 20\nmodel_train_eval = ModelTrainingEvaluation(learning_rate, batch_size, num_epochs, pretrained=True)\nmodel, optimizer, criterion, scheduler = model_train_eval.create_initial_model()\ntrain_loader, validation_loader, test_loader = model_train_eval.create_3_loaders()\nmodel = model_train_eval.train_model(model, optimizer, criterion, scheduler, num_epochs, train_loader, validation_loader)\ntest_accuracy = model_train_eval.evaluate_model(model, test_loader, True)\nprint(f'Final test Accuracy: {100 * test_accuracy:.2f}%')\n# Save the final model\nmodel_train_eval.save_final_model(model)\n\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(f\"Execution Time: {execution_time} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T21:26:04.029902Z","iopub.execute_input":"2023-10-16T21:26:04.030247Z","iopub.status.idle":"2023-10-16T21:26:22.977049Z","shell.execute_reply.started":"2023-10-16T21:26:04.030217Z","shell.execute_reply":"2023-10-16T21:26:22.975976Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"Epoch [1/20] - Validation Accuracy: 71.26%\nEpoch, Loss: (1, 0.55557781457901)\nEpoch [2/20] - Validation Accuracy: 86.21%\nEpoch, Loss: (2, 0.3470309376716614)\nEpoch [3/20] - Validation Accuracy: 93.10%\nEpoch, Loss: (3, 0.14586007595062256)\nEpoch [4/20] - Validation Accuracy: 97.70%\nEpoch, Loss: (4, 0.10899980366230011)\nEpoch [5/20] - Validation Accuracy: 95.40%\nEpoch, Loss: (5, 0.2517448365688324)\nEpoch [6/20] - Validation Accuracy: 91.95%\nEpoch, Loss: (6, 0.018420424312353134)\nEpoch [7/20] - Validation Accuracy: 90.80%\nEpoch, Loss: (7, 0.03528347238898277)\nEpoch [8/20] - Validation Accuracy: 95.40%\nEpoch, Loss: (8, 0.028441883623600006)\nEpoch [9/20] - Validation Accuracy: 95.40%\nEpoch, Loss: (9, 0.02164718694984913)\nEpoch [10/20] - Validation Accuracy: 95.40%\nEpoch, Loss: (10, 0.0044198427349328995)\nEpoch [11/20] - Validation Accuracy: 96.55%\nEpoch, Loss: (11, 0.008224692195653915)\nEpoch [12/20] - Validation Accuracy: 96.55%\nEpoch, Loss: (12, 0.004466133192181587)\nEpoch [13/20] - Validation Accuracy: 96.55%\nEpoch, Loss: (13, 0.0007647468009963632)\nEpoch [14/20] - Validation Accuracy: 97.70%\nEpoch, Loss: (14, 0.0018103671027347445)\nEpoch [15/20] - Validation Accuracy: 98.85%\nEpoch, Loss: (15, 0.0029117041267454624)\nEpoch [16/20] - Validation Accuracy: 98.85%\nEpoch, Loss: (16, 0.0009857098339125514)\nEpoch [17/20] - Validation Accuracy: 100.00%\nEpoch, Loss: (17, 0.0014882064424455166)\nEpoch [18/20] - Validation Accuracy: 100.00%\nEpoch, Loss: (18, 0.0010627153096720576)\nEpoch [19/20] - Validation Accuracy: 100.00%\nEpoch, Loss: (19, 0.0009266350534744561)\nEpoch [20/20] - Validation Accuracy: 100.00%\nEpoch, Loss: (20, 0.001052004867233336)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAhIAAAHHCAYAAADqJrG+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9IElEQVR4nO3deXRUVdb38V8FkiIkJCEQUkQlgMjUTBoQwoxEEEGZRHF4CKNCBwQCDnlbZdA2ttqCA4PaCIjSD4KIEy1gUHAIAkGcpUFQWiBhMsQESULqvH+wqIciAVJF3VSo/n563bU65946Z98KyK59zrllM8YYAQAAeCHI3wEAAIBLF4kEAADwGokEAADwGokEAADwGokEAADwGokEAADwGokEAADwGokEAADwGokEAADwGokEKqWdO3eqV69eioyMlM1m06pVq3za/88//yybzaZFixb5tN9LWffu3dW9e3ef9Zefn6/Ro0fL4XDIZrNp0qRJPusbQOVBIoFz+umnn3TPPfeoYcOGqlatmiIiItSpUyc9++yz+uOPPywdOzk5Wd98843++te/asmSJWrbtq2l41Wk4cOHy2azKSIiosz3cefOnbLZbLLZbHr66ac97n///v2aPn26tm/f7oNovff4449r0aJFGjdunJYsWaL/+Z//sWSc6dOnu96v8x2+SpJWr16t6dOnl/t6p9OpV199Ve3bt1d0dLRq1Kihxo0ba9iwYdq0aZPH4x8/flzTp0/Xxx9/7PFrAStU9XcAqJzef/99DRkyRHa7XcOGDVOLFi1UVFSkTz/9VPfdd5++++47vfTSS5aM/ccffygzM1N/+ctfNH78eEvGiI+P1x9//KHg4GBL+r+QqlWr6vjx43r33Xd16623up17/fXXVa1aNZ04ccKrvvfv368ZM2aofv36atOmTblft3btWq/GO5f169erQ4cOmjZtmk/7PdugQYPUqFEj18/5+fkaN26cBg4cqEGDBrnaY2NjfTLe6tWrNWfOnHInE/fee6/mzJmj/v37684771TVqlW1Y8cO/etf/1LDhg3VoUMHj8Y/fvy4ZsyYIUk+rSAB3iKRQCl79uzR0KFDFR8fr/Xr16tu3bqucykpKdq1a5fef/99y8Y/dOiQJCkqKsqyMWw2m6pVq2ZZ/xdit9vVqVMn/fOf/yyVSCxdulR9+/bVm2++WSGxHD9+XNWrV1dISIhP+z148KCaN2/us/5Onjwpp9NZKs5WrVqpVatWrp8PHz6scePGqVWrVrrrrrt8Nr43cnJyNHfuXI0ZM6ZU4j179mzXn3XgkmaAs4wdO9ZIMp999lm5ri8uLjYzZ840DRs2NCEhISY+Pt6kpaWZEydOuF0XHx9v+vbtaz755BPTrl07Y7fbTYMGDczixYtd10ybNs1Icjvi4+ONMcYkJye7/v+ZTr/mTGvXrjWdOnUykZGRJiwszDRu3NikpaW5zu/Zs8dIMgsXLnR7XUZGhuncubOpXr26iYyMNDfffLP5/vvvyxxv586dJjk52URGRpqIiAgzfPhwU1BQcMH3Kzk52YSFhZlFixYZu91ufvvtN9e5zZs3G0nmzTffNJLMU0895Tp35MgRM2XKFNOiRQsTFhZmatSoYW644Qazfft21zUfffRRqffvzPvs1q2b+dOf/mS2bt1qunTpYkJDQ83EiRNd57p16+bqa9iwYcZut5e6/169epmoqCizb9++Mu/vXDHs2bPHGGNMTk6OGTlypKlTp46x2+2mVatWZtGiRW59nP79PPXUU2bWrFmmYcOGJigoyHz55ZcXfH8PHTpkJJlp06a5tf/www9m8ODBpmbNmsZut5uEhATz9ttvu11TVFRkpk+fbho1amTsdruJjo42nTp1MmvXrjXGnPrdlXVv55KZmWkklbq/c/ntt9/MxIkTzeWXX25CQkLMlVdeaZ544glTUlLi9r6cfZx9r0BFoiKBUt599101bNhQHTt2LNf1o0eP1uLFi3XLLbdoypQp+uKLL5Senq4ffvhBb731ltu1u3bt0i233KJRo0YpOTlZr7zyioYPH66EhAT96U9/0qBBgxQVFaXJkyfr9ttv14033qjw8HCP4v/uu+/Ur18/tWrVSjNnzpTdbteuXbv02Wefnfd1H374ofr06aOGDRtq+vTp+uOPP/T888+rU6dO2rZtm+rXr+92/a233qoGDRooPT1d27Zt0z/+8Q/VqVNHf/vb38oV56BBgzR27FitXLlSI0eOlHSqGtG0aVNdc801pa7fvXu3Vq1apSFDhqhBgwbKycnRiy++qG7duun7779XXFycmjVrppkzZ+qRRx7R3XffrS5dukiS2+/yyJEj6tOnj4YOHaq77rrrnCX/Z599VuvXr1dycrIyMzNVpUoVvfjii1q7dq2WLFmiuLi4Ml/XrFkzLVmyRJMnT9bll1+uKVOmSJJiYmL0xx9/qHv37tq1a5fGjx+vBg0aaPny5Ro+fLhyc3M1ceJEt74WLlyoEydO6O6775bdbld0dHS53tuzfffdd+rUqZMuu+wyPfjggwoLC9Mbb7yhAQMG6M0339TAgQMlnVpvkZ6ertGjR+vaa69VXl6etm7dqm3btun666/XPffco/3792vdunVasmTJBceNj4+XJC1fvlxDhgxR9erVz3nt8ePH1a1bN+3bt0/33HOP6tWrp88//1xpaWk6cOCAZs+erZiYGM2bN6/U1M2ZFRmgwvk7k0HlcuzYMSPJ9O/fv1zXb9++3Ugyo0ePdmufOnWqkWTWr1/vaouPjzeSzMaNG11tBw8eNHa73UyZMsXVduan0TOVtyIxa9YsI8kcOnTonHGXVZFo06aNqVOnjjly5Iir7auvvjJBQUFm2LBhpcYbOXKkW58DBw40tWrVOueYZ95HWFiYMcaYW265xfTs2dMYY0xJSYlxOBxmxowZZb4HJ06ccH0yPfM+7Ha7mTlzpqtty5YtZVZbjDlVdZBk5s+fX+a5MysSxhizZs0aI8k89thjZvfu3SY8PNwMGDDggvdozP9VoM40e/ZsI8m89tprrraioiKTmJhowsPDTV5enuu+JJmIiAhz8ODBco13WlkViZ49e5qWLVu6VcmcTqfp2LGjueqqq1xtrVu3LhXz2VJSUs5bhTjbsGHDjCRTs2ZNM3DgQPP000+bH374odR1jz76qAkLCzP//ve/3doffPBBU6VKFbN3795z3h/gT+zagJu8vDxJUo0aNcp1/erVqyVJqampbu2nP4WevZaiefPmrk/J0qlPqU2aNNHu3bu9jvlsp9dWvP3223I6neV6zYEDB7R9+3YNHz7c7VNvq1atdP3117vu80xjx451+7lLly46cuSI6z0sjzvuuEMff/yxsrOztX79emVnZ+uOO+4o81q73a6goFN/ZUtKSnTkyBGFh4erSZMm2rZtW7nHtNvtGjFiRLmu7dWrl+655x7NnDlTgwYNUrVq1fTiiy+We6yzrV69Wg6HQ7fffrurLTg4WPfee6/y8/O1YcMGt+sHDx6smJgYr8eTpKNHj2r9+vW69dZb9fvvv+vw4cM6fPiwjhw5ot69e2vnzp3at2+fpFN/dr777jvt3LnzosY808KFC/XCCy+oQYMGeuuttzR16lQ1a9ZMPXv2dI0rnapadOnSRTVr1nTFePjwYSUlJamkpEQbN270WUyAL5FIwE1ERIQk6ffffy/X9b/88ouCgoLcVs1LksPhUFRUlH755Re39nr16pXqo2bNmvrtt9+8jLi02267TZ06ddLo0aMVGxuroUOH6o033jhvUnE6ziZNmpQ616xZMx0+fFgFBQVu7WffS82aNSXJo3u58cYbVaNGDS1btkyvv/662rVrV+q9PM3pdGrWrFm66qqrZLfbVbt2bcXExOjrr7/WsWPHyj3mZZdd5tHCyqefflrR0dHavn27nnvuOdWpU6fcrz3bL7/8oquuusqVEJ3WrFkz1/kzNWjQwOuxTtu1a5eMMXr44YcVExPjdpzeUXLw4EFJ0syZM5Wbm6vGjRurZcuWuu+++/T1119f1PhBQUFKSUlRVlaWDh8+rLffflt9+vTR+vXrNXToUNd1O3fu1AcffFAqxqSkJLcYgcqGNRJwExERobi4OH377bcevc5ms5XruipVqpTZbozxeoySkhK3n0NDQ7Vx40Z99NFHev/99/XBBx9o2bJluu6667R27dpzxuCpi7mX0+x2uwYNGqTFixdr9+7d591S+Pjjj+vhhx/WyJEj9eijjyo6OlpBQUGaNGlSuSsv0qn3xxNffvml6x+xb775xq2aYDVPYy3L6fdm6tSp6t27d5nXnE7eunbtqp9++klvv/221q5dq3/84x+aNWuW5s+fr9GjR190LLVq1dLNN9+sm2++Wd27d9eGDRv0yy+/KD4+Xk6nU9dff73uv//+Ml/buHHjix4fsAKJBErp16+fXnrpJWVmZioxMfG8157+D+DOnTtdnyqlU9vecnNzXYvNfKFmzZrKzc0t1X72p1jp1KfAnj17qmfPnnrmmWf0+OOP6y9/+Ys++ugj1ye8s+9Dknbs2FHq3I8//qjatWsrLCzs4m+iDHfccYdeeeUVBQUFuX1CPduKFSvUo0cPLViwwK09NzdXtWvXdv1c3qSuPAoKCjRixAg1b95cHTt21JNPPqmBAweqXbt2XvUXHx+vr7/+Wk6n060q8eOPP7rO+1rDhg0lnZpCKet3f7bo6GiNGDFCI0aMUH5+vrp27arp06e7Eglfvb9t27bVhg0bdODAAcXHx+vKK69Ufn7+BWP05e8X8AWmNlDK/fffr7CwMI0ePVo5OTmlzv/000969tlnJZ0qzUun9sSf6ZlnnpEk9e3b12dxXXnllTp27JhbqfnAgQOldoYcPXq01GtPP5ipsLCwzL7r1q2rNm3aaPHixW7Jyrfffqu1a9e67tMKPXr00KOPPqoXXnhBDofjnNdVqVKlVLVj+fLlbvPsklwJT1lJl6ceeOAB7d27V4sXL9Yzzzyj+vXrKzk5+Zzv44XceOONys7O1rJly1xtJ0+e1PPPP6/w8HB169btomM+W506ddS9e3e9+OKLOnDgQKnzZz7L4ciRI27nwsPD1ahRI7f79eT9zc7O1vfff1+qvaioSBkZGW7TgrfeeqsyMzO1Zs2aUtfn5ubq5MmTkuTa+eGL3y/gC1QkUMqVV16ppUuX6rbbblOzZs3cnmz5+eefu7brSVLr1q2VnJysl156Sbm5uerWrZs2b96sxYsXa8CAAerRo4fP4ho6dKgeeOABDRw4UPfee6+OHz+uefPmqXHjxm6LDWfOnKmNGzeqb9++io+P18GDBzV37lxdfvnl6ty58zn7f+qpp9SnTx8lJiZq1KhRru2fkZGRHj0S2VNBQUF66KGHLnhdv379NHPmTI0YMUIdO3bUN998o9dff931ifu0K6+8UlFRUZo/f75q1KihsLAwtW/f3uP1BuvXr9fcuXM1bdo013bUhQsXqnv37nr44Yf15JNPetSfJN1999168cUXNXz4cGVlZal+/fpasWKFPvvsM82ePbvci3w9NWfOHHXu3FktW7bUmDFj1LBhQ+Xk5CgzM1O//vqrvvrqK0mnFgN3795dCQkJio6O1tatW7VixQq3J6wmJCRIOvXEyt69e6tKlSrnrCT9+uuvuvbaa3XdddepZ8+ecjgcOnjwoP75z3/qq6++0qRJk1zVpPvuu0/vvPOO+vXr59oSXVBQoG+++UYrVqzQzz//rNq1ays0NFTNmzfXsmXL1LhxY0VHR6tFixZq0aKFJe8dcEH+3TSCyuzf//63GTNmjKlfv74JCQkxNWrUMJ06dTLPP/+82za64uJiM2PGDNOgQQMTHBxsrrjiivM+kOpsZ287PNf2T2NOPWiqRYsWJiQkxDRp0sS89tprpbZ/ZmRkmP79+5u4uDgTEhJi4uLizO233+62re5cD6T68MMPTadOnUxoaKiJiIgwN9100zkfSHX29tKFCxe6PXjpXM7c/nku59r+OWXKFFO3bl0TGhpqOnXqZDIzM8vctvn222+b5s2bm6pVq5b5QKqynNlPXl6eiY+PN9dcc40pLi52u27y5MkmKCjIZGZmnvcezvX7zsnJMSNGjDC1a9c2ISEhpmXLlqV+D+f7M3Ah59oe+dNPP5lhw4YZh8NhgoODzWWXXWb69etnVqxY4brmscceM9dee62JiooyoaGhpmnTpuavf/2rKSoqcl1z8uRJM2HCBBMTE2NsNtt5t4Lm5eWZZ5991vTu3dtcfvnlJjg42NSoUcMkJiaal19+2TidTrfrf//9d5OWlmYaNWpkQkJCTO3atU3Hjh3N008/7RbD559/bhISEkxISAhbQeF3NmM8WBkGAABwBtZIAAAAr5FIAAAAr5FIAAAAr5FIAAAAr5FIAAAAr5FIAAAAr5FIAAAArwXkky2LD/vuK6mBQBIa1+XCFwH/ZU4W7bvwRRfJV/8uBddueOGLKhgVCQAA4LWArEgAAFCpOEv8HYFlSCQAALCacfo7AsuQSAAAYDVn4CYSrJEAAABeoyIBAIDFDFMbAADAa0xtAAAAlEZFAgAAqzG1AQAAvBbAz5FgagMAgABUv3592Wy2UkdKSook6cSJE0pJSVGtWrUUHh6uwYMHKycnx+NxSCQAALCacfrm8MCWLVt04MAB17Fu3TpJ0pAhQyRJkydP1rvvvqvly5drw4YN2r9/vwYNGuTxrdmMMcbjV1VyfGkXUDa+tAsorSK+tKto92af9BPS8FqvXztp0iS999572rlzp/Ly8hQTE6OlS5fqlltukST9+OOPatasmTIzM9WhQ4dy90tFAgCAS0RhYaHy8vLcjsLCwgu+rqioSK+99ppGjhwpm82mrKwsFRcXKykpyXVN06ZNVa9ePWVmZnoUE4kEAAAWM8bpkyM9PV2RkZFuR3p6+gXHX7VqlXJzczV8+HBJUnZ2tkJCQhQVFeV2XWxsrLKzsz26N3ZtAABgNR89kCotLU2pqalubXa7/YKvW7Bggfr06aO4uDifxHEmEgkAAKzmo+dI2O32ciUOZ/rll1/04YcfauXKla42h8OhoqIi5ebmulUlcnJy5HA4POqfqQ0AAALYwoULVadOHfXt29fVlpCQoODgYGVkZLjaduzYob179yoxMdGj/qlIAABgNT89kMrpdGrhwoVKTk5W1ar/909+ZGSkRo0apdTUVEVHRysiIkITJkxQYmKiRzs2JBIJAACs56dHZH/44Yfau3evRo4cWercrFmzFBQUpMGDB6uwsFC9e/fW3LlzPR6D50gA/0V4jgRQWkU8R6Lwh4980o+9WQ+f9ONLVCQAALBaAH+NOIkEAABWC+Bv/2TXBgAA8BoVCQAArMbUBgAA8JYx/tn+WRGY2gAAAF6jIgEAgNUCeLEliQQAAFZjjQQAAPBaAFckWCMBAAC8RkUCAACr+elLuyoCiQQAAFZjagMAAKA0KhIAAFiNXRsAAMBrTG0AAACURkUCAACrMbUBAAC8FsCJBFMbAADAa1QkAACwWCB/jTiJBAAAVgvgqQ0SCQAArMb2TwAAgNKoSAAAYDWmNgAAgNeY2gAAACiNigQAAFZjagMAAHiNqQ0AAIDSqEgAAGA1pjYAAIDXAjiRYGoDAAB4jYoEAABWC+DFliQSAABYLYCnNkgkAACwWgBXJFgjAQAAvEZFAgAAqzG1AQAAvMbUBgAAQGlUJAAAsFoAT21QkQAAwGpOp28OD+3bt0933XWXatWqpdDQULVs2VJbt251nTfG6JFHHlHdunUVGhqqpKQk7dy506MxSCQAAAhAv/32mzp16qTg4GD961//0vfff6+///3vqlmzpuuaJ598Us8995zmz5+vL774QmFhYerdu7dOnDhR7nGY2gAAwGrGVPiQf/vb33TFFVdo4cKFrrYGDRqcEZLR7Nmz9dBDD6l///6SpFdffVWxsbFatWqVhg4dWq5xqEgAAGA1H01tFBYWKi8vz+0oLCwsc8h33nlHbdu21ZAhQ1SnTh1dffXVevnll13n9+zZo+zsbCUlJbnaIiMj1b59e2VmZpb71kgkAAC4RKSnpysyMtLtSE9PL/Pa3bt3a968ebrqqqu0Zs0ajRs3Tvfee68WL14sScrOzpYkxcbGur0uNjbWda48mNoAAMBqPtq1kZb2kFJTU93a7Hb7OYZ0qm3btnr88cclSVdffbW+/fZbzZ8/X8nJyT6JR6IiAQCA9YzTJ4fdbldERITbca5Eom7dumrevLlbW7NmzbR3715JksPhkCTl5OS4XZOTk+M6Vx4kEgAAWM0P2z87deqkHTt2uLX9+9//Vnx8vKRTCy8dDocyMjJc5/Py8vTFF18oMTGx3OMwtQEAQACaPHmyOnbsqMcff1y33nqrNm/erJdeekkvvfSSJMlms2nSpEl67LHHdNVVV6lBgwZ6+OGHFRcXpwEDBpR7HBIJAACs5oftn+3atdNbb72ltLQ0zZw5Uw0aNNDs2bN15513uq65//77VVBQoLvvvlu5ubnq3LmzPvjgA1WrVq3c49iM8cPdWaz48G5/hwBUSqFxXfwdAlDpnCzaZ/kYfyy83yf9hI540if9+BJrJAAAgNeY2gAAwGoB/KVdJBIAAFjNBG4iwdQGAADwGhUJAAAsZpwBt6/BhUQCAACrBfAaCaY2AACA16hIAABgtQBebEkiAQCA1VgjAQAAvMYaCQAAgNKoSAAAYLUArkiQSAAAYLXA+35MF6Y2AACA16hI4KL1Gpys/dkHS7UPHdRPD01J0Ywnn1Pmli916PBRVa9eTW1aNNfkP49Uw/gr/BAt4F/jxiZrSuo4ORwx+vrr7zVx0sPasnW7v8OC1ZjaAM7tf//xrJxn/CXZufsXjZn0/9SrRxdJUvMmjdS3Vw/Vja2jY3m/a+6C13T35L9ozfKFqlKlir/CBirckCE36+mnpunPKQ9q85Yvde+E0Vr9/utq3qKrDh064u/wYKUA3v7J1AYuWnTNKNWuFe06Nnz2ha64rK7aXd1SkjSk/41q26alLqsbq+ZNGmnC3cnKzjmkfQdy/Bw5ULEmTxyjfyxYqsWvvqEfftipP6c8qOPH/9CI4UP9HRrgNb9WJA4fPqxXXnlFmZmZys7OliQ5HA517NhRw4cPV0xMjD/DgxeKi4v13tqPNOy2gbLZbKXOH//jhFa9v1aXxzlUN5bfL/57BAcH65prWumJJ19wtRljlLH+U3XokODHyFAhAvjJln6rSGzZskWNGzfWc889p8jISHXt2lVdu3ZVZGSknnvuOTVt2lRbt271V3jwUsbGTP2en68BN17v1v6/K99Tu6SBujZpoD7dtFUvzfqrgoOD/RQlUPFq145W1apVdTDnsFv7wYOH5CCpDnxO45ujEvJbRWLChAkaMmSI5s+fX+qTqzFGY8eO1YQJE5SZmXnefgoLC1VYWOjWFlRYKLvd7vOYcWEr31ujzh3aqk5MLbf2vr16KLHd1Tp05KgWLX1TUx9J15J5f5fdHuKnSAEAvuC3isRXX32lyZMnl1n+ttlsmjx5srZv337BftLT0xUZGel2/O3Z+RZEjAvZn52jTVu3a/BNN5Q6VyM8TPFXXKa2bVpq1l//oj2//EcZGz/3Q5SAfxw+fFQnT55Undjabu116sQoO+eQn6JCRTFOp0+OyshviYTD4dDmzZvPeX7z5s2KjY29YD9paWk6duyY2/HAxLG+DBXl9Nb76xRdM1JdE68973XGGBkjFRUVV1BkgP8VFxdr27avdV2Pzq42m82m63p01qZNWX6MDBWCqQ3fmzp1qu6++25lZWWpZ8+erqQhJydHGRkZevnll/X0009fsB+73V5qGqO46PA5roZVnE6nVr2/Tv37JKlq1f/b0vmffQf0QcZGdbz2GkVHRSr70GEtWPKG7PYQdenYzo8RAxVv1rMva+GCWcra9rW2bPlS904Yo7CwUC1avMzfocFqAbzY0m+JREpKimrXrq1Zs2Zp7ty5KikpkSRVqVJFCQkJWrRokW699VZ/hQcPZW75UgdyDmpg315u7faQEG376lsteWOV8n7PV63oKLVt3UKvzX9GtWpG+SdYwE+WL39HMbWjNf2RqXI4YvTVV9+pb7+7dPAgH35w6bIZ4/8HgBcXF+vw4VN/kWrXrn3Rq/mLD+/2RVhAwAmN6+LvEIBK52TRPsvHKJh5p0/6CXvkdZ/040uV4smWwcHBqlu3rr/DAADAGpV0oaQv8GRLAADgtUpRkQAAIKBV0h0XvkAiAQCA1QJ41wZTGwAAwGtUJAAAsBpTGwAAwFuV9fHWvsDUBgAA8BoVCQAArMbUBgAA8BqJBAAA8BrbPwEAAEqjIgEAgNWY2gAAAN4yAZxIMLUBAEAAmj59umw2m9vRtGlT1/kTJ04oJSVFtWrVUnh4uAYPHqycnByPxyGRAADAak7jm8NDf/rTn3TgwAHX8emnn7rOTZ48We+++66WL1+uDRs2aP/+/Ro0aJDHYzC1AQCA1fz0ZMuqVavK4XCUaj927JgWLFigpUuX6rrrrpMkLVy4UM2aNdOmTZvUoUOHco9BRQIAgAC1c+dOxcXFqWHDhrrzzju1d+9eSVJWVpaKi4uVlJTkurZp06aqV6+eMjMzPRqDigQAAFbz0WLLwsJCFRYWurXZ7XbZ7fZS17Zv316LFi1SkyZNdODAAc2YMUNdunTRt99+q+zsbIWEhCgqKsrtNbGxscrOzvYoJioSAABYzUdrJNLT0xUZGel2pKenlzlknz59NGTIELVq1Uq9e/fW6tWrlZubqzfeeMOnt0YiAQDAJSItLU3Hjh1zO9LS0sr12qioKDVu3Fi7du2Sw+FQUVGRcnNz3a7Jyckpc03F+ZBIAABgMWOMTw673a6IiAi3o6xpjbLk5+frp59+Ut26dZWQkKDg4GBlZGS4zu/YsUN79+5VYmKiR/fGGgkAAKzmhwdSTZ06VTfddJPi4+O1f/9+TZs2TVWqVNHtt9+uyMhIjRo1SqmpqYqOjlZERIQmTJigxMREj3ZsSCQSAABYzw+JxK+//qrbb79dR44cUUxMjDp37qxNmzYpJiZGkjRr1iwFBQVp8ODBKiwsVO/evTV37lyPx7EZYwLuuZ3Fh3f7OwSgUgqN6+LvEIBK52TRPsvHyBt1vU/6iViwzif9+BIVCQAALBbI37VBIgEAgNUCOJFg1wYAAPAaFQkAAKzmn6/aqBAkEgAAWCyQ10gwtQEAALxGRQIAAKsFcEWCRAIAAKsF8BoJpjYAAIDXqEgAAGCxQF5sSSIBAIDVAnhqg0QCAACLBXJFgjUSAADAa1QkAACwGlMbAADAWyaAEwmmNgAAgNeoSAAAYLUArkiQSAAAYDGmNgAAAMpARQIAAKsFcEWCRAIAAIsF8tQGiQQAABYL5ESCNRIAAMBrVCQAALBYIFckSCQAALCasfk7AsswtQEAALxGRQIAAIsxtQEAALxmnExtAAAAlEJFAgAAizG1AQAAvGbYtQEAAFAaFQkAACzG1AYAAPBaIO/aIJEAAMBixvg7AuuwRgIAAHiNigQAABZjagMAAHgtkBMJpjYAAIDXqEgAAGAxFlsCAACvGafNJ8fFeOKJJ2Sz2TRp0iRX24kTJ5SSkqJatWopPDxcgwcPVk5Ojkf9kkgAABDgtmzZohdffFGtWrVya588ebLeffddLV++XBs2bND+/fs1aNAgj/omkQAAwGLG2HxyeCM/P1933nmnXn75ZdWsWdPVfuzYMS1YsEDPPPOMrrvuOiUkJGjhwoX6/PPPtWnTpnL3TyIBAIDFjNM3R2FhofLy8tyOwsLC846dkpKivn37Kikpya09KytLxcXFbu1NmzZVvXr1lJmZWe57I5EAAOASkZ6ersjISLcjPT39nNf/7//+r7Zt21bmNdnZ2QoJCVFUVJRbe2xsrLKzs8sdE7s2AACwmNNHXyOelpam1NRUtza73V7mtf/5z380ceJErVu3TtWqVfPJ+GUhkQAAwGLerm84m91uP2ficLasrCwdPHhQ11xzjautpKREGzdu1AsvvKA1a9aoqKhIubm5blWJnJwcORyOcsdEIgEAgMX88WTLnj176ptvvnFrGzFihJo2baoHHnhAV1xxhYKDg5WRkaHBgwdLknbs2KG9e/cqMTGx3OOQSAAAEIBq1KihFi1auLWFhYWpVq1arvZRo0YpNTVV0dHRioiI0IQJE5SYmKgOHTqUexyvFlt+8sknuuuuu5SYmKh9+/ZJkpYsWaJPP/3Um+4AAAhoxvjm8LVZs2apX79+Gjx4sLp27SqHw6GVK1d61IfHicSbb76p3r17KzQ0VF9++aVr28mxY8f0+OOPe9odAAABrzI82VKSPv74Y82ePdv1c7Vq1TRnzhwdPXpUBQUFWrlypUfrIyQvEonHHntM8+fP18svv6zg4GBXe6dOnbRt2zZPuwMAAJcwj9dI7NixQ127di3VHhkZqdzcXF/EBABAQPHV9s/KyOOKhMPh0K5du0q1f/rpp2rYsKFPggIAIJD48xHZVvM4kRgzZowmTpyoL774QjabTfv379frr7+uqVOnaty4cVbECAAAKimPpzYefPBBOZ1O9ezZU8ePH1fXrl1lt9s1depUTZgwwYoYAQC4pFmx46KysBnj3e0VFRVp165dys/PV/PmzRUeHu7r2LxWfHi3v0MAKqXQuC7+DgGodE4W7bN8jO3xN/uknza/vOOTfnzJ6wdShYSEqHnz5r6MBQAAXGI8TiR69Oghm+3cCz7Wr19/UQEBABBoKutCSV/wOJFo06aN28/FxcXavn27vv32WyUnJ/sqLgAAAkYgr5HwOJGYNWtWme3Tp09Xfn7+RQcEAECg4TkS5XDXXXfplVde8VV3AADgEuCzb//MzMxUtWrVfNXdRWFlOlC2Xo7W/g4B+K/EGokzDBo0yO1nY4wOHDigrVu36uGHH/ZZYAAABIpAntrwOJGIjIx0+zkoKEhNmjTRzJkz1atXL58FBgAAKj+PEomSkhKNGDFCLVu2VM2aNa2KCQCAgBLAmzY8W2xZpUoV9erVi2/5BADAA05j88lRGXm8a6NFixbavZtHUAMAAC8Siccee0xTp07Ve++9pwMHDigvL8/tAAAA7gL5a8TLvUZi5syZmjJlim688UZJ0s033+z2qGxjjGw2m0pKSnwfJQAAlzCnvwOwULkTiRkzZmjs2LH66KOPrIwHAABcQsqdSJz+tvFu3bpZFgwAAIHIqHJOS/iCR9s/z/etnwAAoGzOAN7/6VEi0bhx4wsmE0ePHr2ogAAACDROKhKnzJgxo9STLQEAwH8vjxKJoUOHqk6dOlbFAgBAQGKNhFgfAQCAtwJ5+2e5H0h1etcGAADAaeWuSDidgZxPAQBgHaY2AACA1wL5o7jH37UBAABwGhUJAAAsFsgVCRIJAAAsFshrJJjaAAAAXqMiAQCAxZyBW5AgkQAAwGp81wYAAPBaID/SkTUSAADAa1QkAACwGNs/AQCA15wB/MWXTG0AAACvkUgAAGAx46PDE/PmzVOrVq0UERGhiIgIJSYm6l//+pfr/IkTJ5SSkqJatWopPDxcgwcPVk5Ojsf3RiIBAIDFnD46PHH55ZfriSeeUFZWlrZu3arrrrtO/fv313fffSdJmjx5st59910tX75cGzZs0P79+zVo0CCP781mjAm4XSlVQy7zdwhApdTL0drfIQCVzuq9qy0fY1ndO33Sz20HXr+o10dHR+upp57SLbfcopiYGC1dulS33HKLJOnHH39Us2bNlJmZqQ4dOpS7TxZbAgBgMV892bKwsFCFhYVubXa7XXa7/byvKykp0fLly1VQUKDExERlZWWpuLhYSUlJrmuaNm2qevXqeZxIMLUBAIDFnLL55EhPT1dkZKTbkZ6efs5xv/nmG4WHh8tut2vs2LF666231Lx5c2VnZyskJERRUVFu18fGxio7O9uje6MiAQDAJSItLU2pqalubeerRjRp0kTbt2/XsWPHtGLFCiUnJ2vDhg0+jYlEAgAAi/lqMWJ5pjHOFBISokaNGkmSEhIStGXLFj377LO67bbbVFRUpNzcXLeqRE5OjhwOh0cxMbUBAIDFnDbfHBcdh9OpwsJCJSQkKDg4WBkZGa5zO3bs0N69e5WYmOhRn1QkAACwmD8ekZ2WlqY+ffqoXr16+v3337V06VJ9/PHHWrNmjSIjIzVq1CilpqYqOjpaERERmjBhghITEz1aaCmRSAAAEJAOHjyoYcOG6cCBA4qMjFSrVq20Zs0aXX/99ZKkWbNmKSgoSIMHD1ZhYaF69+6tuXPnejwOz5EA/ovwHAmgtIp4jsTCy+7yST8j9r3mk358iYoEAAAW89VzJCojFlsCAACvUZEAAMBi/lhsWVFIJAAAsFggJxJMbQAAAK9RkQAAwGImgBdbkkgAAGAxpjYAAADKQEUCAACLBXJFgkQCAACLBdwjpM9AIgEAgMV4siUAAEAZqEgAAGAx1kgAAACvBXIiwdQGAADwGhUJAAAsxq4NAADgNXZtAAAAlIGKBAAAFgvkxZYkEgAAWCyQ10gwtQEAALxGRQIAAIs5A7gmQSIBAIDFWCMBAAC8Frj1CNZIAACAi0BFAgAAizG1AQAAvMaTLQEAAMpARQIAAIux/RMAAHgtcNMIpjYAAMBFoCIBAIDF2LUBAAC8FshrJJjaAAAAXqMiAQCAxQK3HkEiAQCA5VgjAQAAvMYaCQAAgDJQkQAAwGKBW4+gIgEAgOWcPjo8kZ6ernbt2qlGjRqqU6eOBgwYoB07drhdc+LECaWkpKhWrVoKDw/X4MGDlZOT49E4JBIAAASgDRs2KCUlRZs2bdK6detUXFysXr16qaCgwHXN5MmT9e6772r58uXasGGD9u/fr0GDBnk0DlMbAABYzPhhcuODDz5w+3nRokWqU6eOsrKy1LVrVx07dkwLFizQ0qVLdd1110mSFi5cqGbNmmnTpk3q0KFDucahIgEAgMV8NbVRWFiovLw8t6OwsLBcMRw7dkySFB0dLUnKyspScXGxkpKSXNc0bdpU9erVU2ZmZrnvjUQCAIBLRHp6uiIjI92O9PT0C77O6XRq0qRJ6tSpk1q0aCFJys7OVkhIiKKiotyujY2NVXZ2drljYmoDAACL+eo5EmlpaUpNTXVrs9vtF3xdSkqKvv32W3366ac+ieNMJBIAAFjMVysk7HZ7uRKHM40fP17vvfeeNm7cqMsvv9zV7nA4VFRUpNzcXLeqRE5OjhwOR7n7Z2oDAIAAZIzR+PHj9dZbb2n9+vVq0KCB2/mEhAQFBwcrIyPD1bZjxw7t3btXiYmJ5R6HRAKWGTc2Wbv+vUn5eT/p80/fVbu2bfwdElChWlzbQtNemaYlW5Zo9d7VSux17v84j398vFbvXa3+o/pXYISoKE4ZnxyeSElJ0WuvvaalS5eqRo0ays7OVnZ2tv744w9JUmRkpEaNGqXU1FR99NFHysrK0ogRI5SYmFjuHRsSiQQsMmTIzXr6qWl69LFn1K79Dfrq6++1+v3XFRNTy9+hARWmWvVq2vP9Hs19aO55r0vsnagmVzfR4ezDFRQZKpo/Hkg1b948HTt2TN27d1fdunVdx7Jly1zXzJo1S/369dPgwYPVtWtXORwOrVy50qNxSCRgickTx+gfC5Zq8atv6IcfdurPKQ/q+PE/NGL4UH+HBlSYrR9v1atPv6rMNefeSlcrtpbGzRynpyY+pZLikgqMDhXJ+Oh/Ho1pTJnH8OHDXddUq1ZNc+bM0dGjR1VQUKCVK1d6tD5CIpGABYKDg3XNNa2Usf4TV5sxRhnrP1WHDgl+jAyoXGw2m6bOnqo3X3xTe/+919/hAF6p1InEf/7zH40cOfK815T1cA5jAvnrUSq/2rWjVbVqVR3McS/THjx4SI7YGD9FBVQ+Q/48RCUlJXr7lbf9HQos5o+pjYpSqROJo0ePavHixee9pqyHcxjn7xUUIQB4p1HLRrp5xM16Zsoz/g4FFcAfUxsVxa/PkXjnnXfOe3737t0X7KOsh3PUrNX0ouLCxTl8+KhOnjypOrG13drr1IlRds4hP0UFVC5/uvZPiqodpcWZ//dhqUrVKhr90GgNGDlAIzqN8GN0QPn5NZEYMGCAbDbbeacibDbbefso6+EcF3oNrFVcXKxt277WdT0665131kg69Tu5rkdnzZ230M/RAZXD+jfXa/sn293aHn3tUa1fuV7r3ljnn6Bgmco6LeELfk0k6tatq7lz56p//7L3TW/fvl0JCSzOuxTNevZlLVwwS1nbvtaWLV/q3gljFBYWqkWLl134xUCAqFa9muLqx7l+jr0iVg2bN9Tvub/r0P5D+j3XfRq2pLhEvx36Tft276voUGExZwCv3fNrIpGQkKCsrKxzJhIXqlag8lq+/B3F1I7W9EemyuGI0Vdffae+/e7SwYPsk8d/j6taXaW/vfE31893T7tbkrRu+TrNmjLLX2EBPmUzfvyX+pNPPlFBQYFuuOGGMs8XFBRo69at6tatm0f9Vg25zBfhAQGnl6O1v0MAKp3Ve1dbPsZd8YN80s9rv3j2sKiK4NeKRJcuXc57PiwszOMkAgCAysZX3/5ZGVXq7Z8AAKBy42vEAQCwWGV9BoQvkEgAAGAxtn8CAACvsUYCAACgDFQkAACwGGskAACA1wJ5jQRTGwAAwGtUJAAAsFggf90DiQQAABZj1wYAAEAZqEgAAGCxQF5sSSIBAIDFAnn7J1MbAADAa1QkAACwWCAvtiSRAADAYmz/BAAAXgvkxZaskQAAAF6jIgEAgMUCedcGiQQAABYL5MWWTG0AAACvUZEAAMBi7NoAAABeY2oDAACgDFQkAACwGLs2AACA15wBvEaCqQ0AAOA1KhIAAFgscOsRJBIAAFgukHdtkEgAAGCxQE4kWCMBAAC8RiIBAIDFjDE+OTy1ceNG3XTTTYqLi5PNZtOqVatKxfXII4+obt26Cg0NVVJSknbu3OnRGCQSAABYzCnjk8NTBQUFat26tebMmVPm+SeffFLPPfec5s+fry+++EJhYWHq3bu3Tpw4Ue4xWCMBAECA6tOnj/r06VPmOWOMZs+erYceekj9+/eXJL366quKjY3VqlWrNHTo0HKNQUUCAACLGR/9r7CwUHl5eW5HYWGhVzHt2bNH2dnZSkpKcrVFRkaqffv2yszMLHc/JBIAAFjMV2sk0tPTFRkZ6Xakp6d7FVN2drYkKTY21q09NjbWda48mNoAAOASkZaWptTUVLc2u93up2hOIZEAAMBivnqOhN1u91ni4HA4JEk5OTmqW7euqz0nJ0dt2rQpdz9MbQAAYDF/bf88nwYNGsjhcCgjI8PVlpeXpy+++EKJiYnl7oeKBAAAASo/P1+7du1y/bxnzx5t375d0dHRqlevniZNmqTHHntMV111lRo0aKCHH35YcXFxGjBgQLnHIJEAAMBi/npE9tatW9WjRw/Xz6fXVyQnJ2vRokW6//77VVBQoLvvvlu5ubnq3LmzPvjgA1WrVq3cY9iMr2sllUDVkMv8HQJQKfVytPZ3CECls3rvasvHaOUo/1TB+XydXf5tmRWFigQAABZzBt5ndhcWWwIAAK9RkQAAwGImgL9GnEQCAACLMbUBAABQBioSAABYjKkNAADgNaY2AAAAykBFAgAAizG1AQAAvMbUBgAAQBmoSAAAYDGmNgAAgNeMcfo7BMuQSAAAYDF/fY14RWCNBAAA8BoVCQAALGYCeNcGiQQAABZjagMAAKAMVCQAALAYUxsAAMBrPNkSAACgDFQkAACwGE+2BAAAXgvkNRJMbQAAAK9RkQAAwGKB/BwJEgkAACwWyFMbJBIAAFiM7Z8AAABloCIBAIDFmNoAAABeC+TFlkxtAAAAr1GRAADAYkxtAAAAr7FrAwAAoAxUJAAAsBhf2gUAALzG1AYAAEAZqEgAAGAxdm0AAACvBfIaCaY2AACwmDHGJ4c35syZo/r166tatWpq3769Nm/e7NN7I5EAACBALVu2TKmpqZo2bZq2bdum1q1bq3fv3jp48KDPxiCRAADAYv6qSDzzzDMaM2aMRowYoebNm2v+/PmqXr26XnnlFZ/dG4kEAAAWMz46PFFUVKSsrCwlJSW52oKCgpSUlKTMzMyLup8zsdgSAIBLRGFhoQoLC93a7Ha77HZ7qWsPHz6skpISxcbGurXHxsbqxx9/9FlMAZlInCza5+8QoFN/4NPT05WWllbmH3LgvxV/N/77+OrfpenTp2vGjBlubdOmTdP06dN90r83bCaQN7fCr/Ly8hQZGaljx44pIiLC3+EAlQZ/N+AtTyoSRUVFql69ulasWKEBAwa42pOTk5Wbm6u3337bJzGxRgIAgEuE3W5XRESE23GuqlZISIgSEhKUkZHhanM6ncrIyFBiYqLPYgrIqQ0AACClpqYqOTlZbdu21bXXXqvZs2eroKBAI0aM8NkYJBIAAASo2267TYcOHdIjjzyi7OxstWnTRh988EGpBZgXg0QClrHb7Zo2bRqLyYCz8HcDFWn8+PEaP368Zf2z2BIAAHiNxZYAAMBrJBIAAMBrJBIAAMBrJBIAAMBrJBKwzJw5c1S/fn1Vq1ZN7du31+bNm/0dEuBXGzdu1E033aS4uDjZbDatWrXK3yEBF41EApZYtmyZUlNTNW3aNG3btk2tW7dW7969dfDgQX+HBvhNQUGBWrdurTlz5vg7FMBn2P4JS7Rv317t2rXTCy+8IOnUY1mvuOIKTZgwQQ8++KCfowP8z2az6a233nL7DgTgUkRFAj5XVFSkrKwsJSUludqCgoKUlJSkzMxMP0YGAPA1Egn43OHDh1VSUlLqEayxsbHKzs72U1QAACuQSAAAAK+RSMDnateurSpVqignJ8etPScnRw6Hw09RAQCsQCIBnwsJCVFCQoIyMjJcbU6nUxkZGUpMTPRjZAAAX+PbP2GJ1NRUJScnq23btrr22ms1e/ZsFRQUaMSIEf4ODfCb/Px87dq1y/Xznj17tH37dkVHR6tevXp+jAzwHts/YZkXXnhBTz31lLKzs9WmTRs999xzat++vb/DAvzm448/Vo8ePUq1Jycna9GiRRUfEOADJBIAAMBrrJEAAABeI5EAAABeI5EAAABeI5EAAABeI5EAAABeI5EAAABeI5EAAABeI5EAAtDw4cM1YMAA18/du3fXpEmTKjyOjz/+WDabTbm5uRU+NoCKQSIBVKDhw4fLZrPJZrMpJCREjRo10syZM3Xy5ElLx125cqUeffTRcl3LP/4APMF3bQAV7IYbbtDChQtVWFio1atXKyUlRcHBwUpLS3O7rqioSCEhIT4ZMzo62if9AMDZqEgAFcxut8vhcCg+Pl7jxo1TUlKS3nnnHdd0xF//+lfFxcWpSZMmkqT//Oc/uvXWWxUVFaXo6Gj1799fP//8s6u/kpISpaamKioqSrVq1dL999+vs598f/bURmFhoR544AFdccUVstvtatSokRYsWKCff/7Z9V0QNWvWlM1m0/DhwyWd+gbX9PR0NWjQQKGhoWrdurVWrFjhNs7q1avVuHFjhYaGqkePHm5xAghMJBKAn4WGhqqoqEiSlJGRoR07dmjdunV67733VFxcrN69e6tGjRr65JNP9Nlnnyk8PFw33HCD6zV///vftWjRIr3yyiv69NNPdfToUb311lvnHXPYsGH65z//qeeee04//PCDXnzxRYWHh+uKK67Qm2++KUnasWOHDhw4oGeffVaSlJ6erldffVXz58/Xd999p8mTJ+uuu+7Shg0bJJ1KeAYNGqSbbrpJ27dv1+jRo/Xggw9a9bYBqCwMgAqTnJxs+vfvb4wxxul0mnXr1hm73W6mTp1qkpOTTWxsrCksLHRdv2TJEtOkSRPjdDpdbYWFhSY0NNSsWbPGGGNM3bp1zZNPPuk6X1xcbC6//HLXOMYY061bNzNx4kRjjDE7duwwksy6devKjPGjjz4yksxvv/3majtx4oSpXr26+fzzz92uHTVqlLn99tuNMcakpaWZ5s2bu51/4IEHSvUFILCwRgKoYO+9957Cw8NVXFwsp9OpO+64Q9OnT1dKSopatmzpti7iq6++0q5du1SjRg23Pk6cOKGffvpJx44d04EDB9y+nr1q1apq27ZtqemN07Zv364qVaqoW7du5Y55165dOn78uK6//nq39qKiIl199dWSpB9++KHU18QnJiaWewwAlyYSCaCC9ejRQ/PmzVNISIji4uJUter//TUMCwtzuzY/P18JCQl6/fXXS/UTExPj1fihoaEevyY/P1+S9P777+uyyy5zO2e3272KA0BgIJEAKlhYWJgaNWpUrmuvueYaLVu2THXq1FFERESZ19StW1dffPGFunbtKkk6efKksrKydM0115R5fcuWLeV0OrVhwwYlJSWVOn+6IlJSUuJqa968uex2u/bu3XvOSkazZs30zjvvuLVt2rTpwjcJ4JLGYkugErvzzjtVu3Zt9e/fX5988on27Nmjjz/+WPfee69+/fVXSdLEiRP1xBNPaNWqVfrxxx/15z//+bzPgKhfv76Sk5M1cuRIrVq1ytXnG2+8IUmKj4+XzWbTe++9p0OHDik/P181atTQ1KlTNXnyZC1evFg//fSTtm3bpueff16LFy+WJI0dO1Y7d+7Ufffdpx07dmjp0qVatGiR1W8RAD8jkQAqserVq2vjxo2qV6+eBg0apGbNmmnUqFE6ceKEq0IxZcoU/c///I+Sk5OVmJioGjVqaODAgeftd968ebrlllv05z//WU2bNtWYMWNUUFAgSbrssss0Y8YMPfjgg4qNjdX48eMlSY8++qgefvhhpaenq1mzZrrhhhv0/vvvq0GDBpKkevXq6c0339SqVavUunVrzZ8/X48//riF7w6AysBmzrUiCwAA4AKoSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK+RSAAAAK/9f2ZE6rUXUePfAAAAAElFTkSuQmCC"},"metadata":{}},{"name":"stdout","text":"Final test Accuracy: 100.00%\nExecution Time: 18.933236122131348 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}